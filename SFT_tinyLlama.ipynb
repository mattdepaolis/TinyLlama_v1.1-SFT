{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8be49fd",
   "metadata": {},
   "source": [
    "# Fine-tuning for Memorization via Chat Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4214783",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0abf6bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_HUB_ENABLEHF_TRANSFER=True\n"
     ]
    }
   ],
   "source": [
    "%env HF_HUB_ENABLEHF_TRANSFER = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf9f1571",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_API_KEY=YOUR_TOKEN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatthias-depaolis\u001b[0m (\u001b[33mdemaz\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install wandb -q -U\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "#%env WANDB_NOTEBOOK_NAME = $Fine_tune_tinyllama_with_DPO\n",
    "wandb.login(key=os.environ[\"WANDB_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3da65936",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install --upgrade pip\n",
    "#!pip install -U -q transformers\n",
    "#!pip install -U -q bitsandbytes\n",
    "#!pip install -U -q peft\n",
    "#!pip install -U -q accelerate\n",
    "#!pip install -U -q scipy\n",
    "#!pip install -U -q trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd81aed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = ''\n",
    "\n",
    "model_id = \"./TinyLlama/TinyLlama_v1.1\"\n",
    "new_model = \"./llmat/TinyLlama-1.1B_SFT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abf34444",
   "metadata": {},
   "outputs": [],
   "source": [
    "## monitor gpu activity\n",
    "# watch -n 0.5 nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de96619d",
   "metadata": {},
   "source": [
    "## Load the Model and Tokenizer for LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f7c0c5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in oss file\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    #config=config,\n",
    "    #quantization_config=bnb_config,\n",
    "    #rope_scaling={'type': 'Linear', 'factor': 2.0},\n",
    "    device_map='auto',\n",
    "    attn_implementation = \"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    cache_dir=cache_dir\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True, trust_remote_code=True, cache_dir=cache_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e434c1",
   "metadata": {},
   "source": [
    "## Loading Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bde9c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check there are no parameters overflowing onto cpu (meta).\n",
    "for n, p in model.named_parameters():\n",
    "    if p.device.type=='meta':\n",
    "        print(f\"{n} is on meta!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea0b250a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(model.config.max_position_embeddings)\n",
    "print(model.config.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28d74c1",
   "metadata": {},
   "source": [
    "# Prepare for LoRA fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32575f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainabl√∂e parameters in the model and lists which parameters\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    non_trainable_params = 0\n",
    "    all_params = 0\n",
    "\n",
    "    print(\"Trainable Parameters\")\n",
    "    for name, param in model.named_parameters():\n",
    "        all_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "            print(f\" {name}\")\n",
    "        else:\n",
    "            non_trainable_params += param.numel()\n",
    "\n",
    "    print(\"\\nNon-Trainable Parameters:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            print(f\" {name}\")\n",
    "\n",
    "    print(\n",
    "        f\"\\nSummary:\\n Trainable params: {trainable_params}\\n Non-Trainable params: {non_trainable_params}\\n All Parameters: {all_params}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e3d49e",
   "metadata": {},
   "source": [
    "# Standard LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d73a99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-21): 22 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaFlashAttention2(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29118ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable() # Comment this in to save VRAM\n",
    "# model = prepare_model_for_kbit_training(model) # only set this if using quantization\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig( # matching the Llama recipe\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        # \"self_attn.rotary_emb.inv_freq\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\", # Language model head - best to set this trainable if chat fine-tuning\n",
    "        #\"lora_magnitude_vector\", # required for DoRA\n",
    "        #\"input_layernorm.weight\", #can't be lora fine-tuned as it's not a linear layer\n",
    "        #\"post_attention_layernorm.weights, #can't be lora fine-tuned as it's not a linear layer\n",
    "        #\"model.norm.weight\", #can't be lora fine-tuned as it's not a linear layer\n",
    "        #\"dense_h_to_4h\", # for falcon\n",
    "        #\"dense_4h_to_h\", # for falcon\n",
    "        #\"query_key_value\", # for falcon\n",
    "        #\"dense\", # for falcon\n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    #use_dora=True # only for DoRA\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config) # move to a peft model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71234548",
   "metadata": {},
   "source": [
    "# Set up Tokenizer and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1af1d5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaTokenizerFast(name_or_path='./TinyLlama/TinyLlama_v1.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "32000\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00389537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>\n",
      "</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.bos_token)\n",
    "print(tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8a5ff87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d19f5a21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] write a quick sorf algorithm in python. [/INST]here you are.</s>[INST] great. [/INST]\n"
     ]
    }
   ],
   "source": [
    "# OPTIONALLY SET THE CHAT TEMPLATE MANUALLY\n",
    "# Llama/Mistral template. NOTE: This is a special chat template that includes a check to add a beginning-of-sequence token (bos_token) if the first message in the conversation is not from the assistant. This is done to ensure that the conversation starts correctly depending on the initial message role.\n",
    "#¬†This is done because we are separatly format the chosen and rejected responses. When we do that there is not going to be a user message at the beginning and do not want to add an extra bos_token before the response\n",
    "tokenizer.chat_template = \"\"\"{% if messages[0]['role'] != 'assistant' %}{{ bos_token }}{% endif %}{% for message in messages %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token }}{% endif %}{% endfor %}\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {'role': 'user', 'content': 'write a quick sorf algorithm in python.'},\n",
    "    {'role': 'assistant', 'content': 'here you are.'},\n",
    "    {'role': 'user', 'content': 'great.'},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a6fa077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk> token is in the tokenizer. Using for <unk> for pad\n"
     ]
    }
   ],
   "source": [
    "##¬†Option A - set the pad token to <pad>, if not <|pad|>, if not <unk> if <unk>\n",
    "if '<pad>' in tokenizer.get_vocab():\n",
    "    print('<pad> token is is in the tokenizer. Usinh <pad> for pad')\n",
    "    #Set the pad token\n",
    "    tokenizer.pad_token = '<pad>'\n",
    "elif '<|pad|>' in tokenizer.get_vocab():\n",
    "    print('<|pad|> token is in the tokenizer. Using for <|pad|> for pad')\n",
    "    # Set the pad token\n",
    "    tokenizer.pad_token = '<|pad|>'\n",
    "elif '<unk>' in tokenizer.get_vocab():\n",
    "    print('<unk> token is in the tokenizer. Using for <unk> for pad')\n",
    "    # Set the pad token\n",
    "    tokenizer.pad_token = '<unk>'\n",
    "else:\n",
    "    print(f'Using EOS token, {tokenizer.eos_token}, for padding. Warning, this ')\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50ca72b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer pad token ID: 0\n",
      "Model pad token ID: 0\n",
      "Model config pad token ID: 0\n",
      "Number of tokens now in tokenizer: 32000\n"
     ]
    }
   ],
   "source": [
    "# Update pad token id in model and its config\n",
    "model.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "#¬†Check if they are equal\n",
    "assert model.pad_token_id == tokenizer.pad_token_id, \"The model's pat token ID are not equal\"\n",
    "\n",
    "# Print the pad token ids\n",
    "print('Tokenizer pad token ID:', tokenizer.pad_token_id)\n",
    "print('Model pad token ID:', model.pad_token_id)\n",
    "print('Model config pad token ID:', model.config.pad_token_id)\n",
    "print('Number of tokens now in tokenizer:', tokenizer.vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d59da02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens map: {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}\n",
      "All special tokens: ['<s>', '</s>', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "print('Special tokens map:', tokenizer.special_tokens_map)\n",
    "print('All special tokens:', tokenizer.all_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd1f2f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Uncomment to switch to left padding, not recommende for unsloth.\n",
    "# tokenizer.padding_side = 'left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3b419ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaTokenizerFast(name_or_path='./TinyLlama/TinyLlama_v1.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55373097",
   "metadata": {},
   "source": [
    "# Set embed and norm layers to trainable (recommended for chat fine-tuning if you are changing the template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0837bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(32000, 2048)\n",
      "        (layers): ModuleList(\n",
      "          (0-21): 22 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaFlashAttention2(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=5632, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=5632, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=5632, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm()\n",
      "            (post_attention_layernorm): LlamaRMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm()\n",
      "      )\n",
      "      (lm_head): lora.Linear(\n",
      "        (base_layer): Linear(in_features=2048, out_features=32000, bias=False)\n",
      "        (lora_dropout): ModuleDict(\n",
      "          (default): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (lora_A): ModuleDict(\n",
      "          (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "        )\n",
      "        (lora_B): ModuleDict(\n",
      "          (default): Linear(in_features=8, out_features=32000, bias=False)\n",
      "        )\n",
      "        (lora_embedding_A): ParameterDict()\n",
      "        (lora_embedding_B): ParameterDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b8a6fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to hold the names of the trainable parameters\n",
    "# trainable_params_names = ['word_embeddings', 'input_layernorm', 'ln_f'] # for Falcon\n",
    "trainable_params_names = ['embed_tokens', 'input_layernorm', 'post_attention_layernorm', 'norm']\n",
    "# trainable_params_names = ['embed', 'norm'] # for DeepSeek Coder\n",
    "\n",
    "# Set modules to be trainable\n",
    "for n, p in model.named_parameters():\n",
    "    if any(k in n for k in trainable_params_names):\n",
    "        p.requires_grad_(True)\n",
    "    else:\n",
    "        p.requires_grad_(False) # Optional: Set the rest to be trainable\n",
    "\n",
    "# Make a dictionary of trainable parameters\n",
    "trainable_params = {n: p for n, p in model.named_parameters() if p.requires_grad}\n",
    "\n",
    "# Convert trainable_params to state_dict format\n",
    "trainable_params_state_dict = {n: p.data for n, p in trainable_params.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98e7582d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable Parameters\n",
      " base_model.model.model.embed_tokens.weight\n",
      " base_model.model.model.layers.0.input_layernorm.weight\n",
      " base_model.model.model.layers.0.post_attention_layernorm.weight\n",
      " base_model.model.model.layers.1.input_layernorm.weight\n",
      " base_model.model.model.layers.1.post_attention_layernorm.weight\n",
      " base_model.model.model.layers.2.input_layernorm.weight\n",
      " base_model.model.model.layers.2.post_attention_layernorm.weight\n",
      " base_model.model.model.layers.3.input_layernorm.weight\n",
      " base_model.model.model.layers.3.post_attention_layernorm.weight\n",
      " base_model.model.model.layers.4.input_layernorm.weight\n",
      " base_model.model.model.layers.4.post_attention_layernorm.weight\n",
      " base_model.model.model.layers.5.input_layernorm.weight\n",
      " base_model.model.model.layers.5.post_attention_layernorm.weight\n",
      " base_model.model.model.layers.6.input_layernorm.weight\n",
      " base_model.model.model.layers.6.post_attention_layernorm.weight\n",
      " base_model.model.model.layers.7.input_layernorm.weight\n",
      " base_model.model.model.layers.7.post_attention_layernorm.weight\n",
      " base_model.model.model.layers.8.input_layernorm.weight\n",
      " base_model.model.model.layers.8.post_attention_layernorm.weight\n",
      " base_model.model.model.layers.9.input_layernorm.weight\n",
      " base_model.model.model.layers.9.post_attention_layernorm.weight\n",
      " base_model.model.model.layers.10.input_layernorm.weight\n",
      " base_model.model.model.layers.10.post_attention_layernorm.weight\n",
      " base_model.model.model.layers.11.input_layernorm.weight\n",
      " base_model.model.model.layers.11.post_attention_layernorm.weight\n",
      " base_model.model.model.layers.12.input_layernorm.weight\n",
      " base_model.model.model.layers.12.post_attention_layernorm.weight\n",
      " base_model.model.model.layers.13.input_layernorm.weight\n",
      " base_model.model.model.layers.13.post_attention_layernorm.weight\n",
      " base_model.model.model.layers.14.input_layernorm.weight\n",
      " base_model.model.model.layers.14.post_attention_layernorm.weight\n",
      " base_model.model.model.layers.15.input_layernorm.weight\n",
      " base_model.model.model.layers.15.post_attention_layernorm.weight\n",
      " base_model.model.model.layers.16.input_layernorm.weight\n",
      " base_model.model.model.layers.16.post_attention_layernorm.weight\n",
      " base_model.model.model.layers.17.input_layernorm.weight\n",
      " base_model.model.model.layers.17.post_attention_layernorm.weight\n",
      " base_model.model.model.layers.18.input_layernorm.weight\n",
      " base_model.model.model.layers.18.post_attention_layernorm.weight\n",
      " base_model.model.model.layers.19.input_layernorm.weight\n",
      " base_model.model.model.layers.19.post_attention_layernorm.weight\n",
      " base_model.model.model.layers.20.input_layernorm.weight\n",
      " base_model.model.model.layers.20.post_attention_layernorm.weight\n",
      " base_model.model.model.layers.21.input_layernorm.weight\n",
      " base_model.model.model.layers.21.post_attention_layernorm.weight\n",
      " base_model.model.model.norm.weight\n",
      "\n",
      "Non-Trainable Parameters:\n",
      " base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight\n",
      " base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight\n",
      " base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight\n",
      " base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight\n",
      " base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight\n",
      " base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.0.mlp.up_proj.base_layer.weight\n",
      " base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.0.mlp.down_proj.base_layer.weight\n",
      " base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight\n",
      " base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight\n",
      " base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight\n",
      " base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight\n",
      " base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.1.mlp.gate_proj.base_layer.weight\n",
      " base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.1.mlp.up_proj.base_layer.weight\n",
      " base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.1.mlp.down_proj.base_layer.weight\n",
      " base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight\n",
      " base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight\n",
      " base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight\n",
      " base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight\n",
      " base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.2.mlp.gate_proj.base_layer.weight\n",
      " base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.2.mlp.up_proj.base_layer.weight\n",
      " base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.2.mlp.down_proj.base_layer.weight\n",
      " base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight\n",
      " base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight\n",
      " base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight\n",
      " base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight\n",
      " base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.3.mlp.gate_proj.base_layer.weight\n",
      " base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.3.mlp.up_proj.base_layer.weight\n",
      " base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.3.mlp.down_proj.base_layer.weight\n",
      " base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight\n",
      " base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight\n",
      " base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight\n",
      " base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight\n",
      " base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.4.mlp.gate_proj.base_layer.weight\n",
      " base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.4.mlp.up_proj.base_layer.weight\n",
      " base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.4.mlp.down_proj.base_layer.weight\n",
      " base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight\n",
      " base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight\n",
      " base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight\n",
      " base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight\n",
      " base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.5.mlp.gate_proj.base_layer.weight\n",
      " base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.5.mlp.up_proj.base_layer.weight\n",
      " base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.5.mlp.down_proj.base_layer.weight\n",
      " base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight\n",
      " base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight\n",
      " base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight\n",
      " base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight\n",
      " base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.6.mlp.gate_proj.base_layer.weight\n",
      " base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.6.mlp.up_proj.base_layer.weight\n",
      " base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.6.mlp.down_proj.base_layer.weight\n",
      " base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight\n",
      " base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight\n",
      " base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight\n",
      " base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight\n",
      " base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.7.mlp.gate_proj.base_layer.weight\n",
      " base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.7.mlp.up_proj.base_layer.weight\n",
      " base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.7.mlp.down_proj.base_layer.weight\n",
      " base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight\n",
      " base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight\n",
      " base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight\n",
      " base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight\n",
      " base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.8.mlp.gate_proj.base_layer.weight\n",
      " base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.8.mlp.up_proj.base_layer.weight\n",
      " base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.8.mlp.down_proj.base_layer.weight\n",
      " base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight\n",
      " base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight\n",
      " base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight\n",
      " base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight\n",
      " base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.9.mlp.gate_proj.base_layer.weight\n",
      " base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.9.mlp.up_proj.base_layer.weight\n",
      " base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.9.mlp.down_proj.base_layer.weight\n",
      " base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight\n",
      " base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight\n",
      " base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight\n",
      " base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight\n",
      " base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.10.mlp.gate_proj.base_layer.weight\n",
      " base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.10.mlp.up_proj.base_layer.weight\n",
      " base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.10.mlp.down_proj.base_layer.weight\n",
      " base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight\n",
      " base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight\n",
      " base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight\n",
      " base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight\n",
      " base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.11.mlp.gate_proj.base_layer.weight\n",
      " base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.11.mlp.up_proj.base_layer.weight\n",
      " base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.11.mlp.down_proj.base_layer.weight\n",
      " base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight\n",
      " base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight\n",
      " base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight\n",
      " base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight\n",
      " base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.12.mlp.gate_proj.base_layer.weight\n",
      " base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.12.mlp.up_proj.base_layer.weight\n",
      " base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.12.mlp.down_proj.base_layer.weight\n",
      " base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight\n",
      " base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight\n",
      " base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight\n",
      " base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight\n",
      " base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.13.mlp.gate_proj.base_layer.weight\n",
      " base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.13.mlp.up_proj.base_layer.weight\n",
      " base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.13.mlp.down_proj.base_layer.weight\n",
      " base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight\n",
      " base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight\n",
      " base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight\n",
      " base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight\n",
      " base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.14.mlp.gate_proj.base_layer.weight\n",
      " base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.14.mlp.up_proj.base_layer.weight\n",
      " base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.14.mlp.down_proj.base_layer.weight\n",
      " base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight\n",
      " base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight\n",
      " base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight\n",
      " base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight\n",
      " base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.15.mlp.gate_proj.base_layer.weight\n",
      " base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.15.mlp.up_proj.base_layer.weight\n",
      " base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.15.mlp.down_proj.base_layer.weight\n",
      " base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight\n",
      " base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight\n",
      " base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight\n",
      " base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight\n",
      " base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.16.mlp.gate_proj.base_layer.weight\n",
      " base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.16.mlp.up_proj.base_layer.weight\n",
      " base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.16.mlp.down_proj.base_layer.weight\n",
      " base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight\n",
      " base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight\n",
      " base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight\n",
      " base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight\n",
      " base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.17.mlp.gate_proj.base_layer.weight\n",
      " base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.17.mlp.up_proj.base_layer.weight\n",
      " base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.17.mlp.down_proj.base_layer.weight\n",
      " base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight\n",
      " base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight\n",
      " base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight\n",
      " base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight\n",
      " base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.18.mlp.gate_proj.base_layer.weight\n",
      " base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.18.mlp.up_proj.base_layer.weight\n",
      " base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.18.mlp.down_proj.base_layer.weight\n",
      " base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight\n",
      " base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight\n",
      " base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight\n",
      " base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight\n",
      " base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.19.mlp.gate_proj.base_layer.weight\n",
      " base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.19.mlp.up_proj.base_layer.weight\n",
      " base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.19.mlp.down_proj.base_layer.weight\n",
      " base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight\n",
      " base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight\n",
      " base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight\n",
      " base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight\n",
      " base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.20.mlp.gate_proj.base_layer.weight\n",
      " base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.20.mlp.up_proj.base_layer.weight\n",
      " base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.20.mlp.down_proj.base_layer.weight\n",
      " base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight\n",
      " base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight\n",
      " base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight\n",
      " base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight\n",
      " base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.21.mlp.gate_proj.base_layer.weight\n",
      " base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.21.mlp.up_proj.base_layer.weight\n",
      " base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight\n",
      " base_model.model.model.layers.21.mlp.down_proj.base_layer.weight\n",
      " base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight\n",
      " base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight\n",
      " base_model.model.lm_head.base_layer.weight\n",
      " base_model.model.lm_head.lora_A.default.weight\n",
      " base_model.model.lm_head.lora_B.default.weight\n",
      "\n",
      "Summary:\n",
      " Trainable params: 65628160\n",
      " Non-Trainable params: 1041000448\n",
      " All Parameters: 1106628608\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f7437b",
   "metadata": {},
   "source": [
    "# Set up Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb0de39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import gc # import Python's garbage collection module\n",
    "\n",
    "# Define a stream\n",
    "def stream(user_prompt, model_type, tokenizer, checkpoint=''):\n",
    "    if model_type == 'base':\n",
    "        eval_model = model\n",
    "    elif model_type == 'fine-tuned':\n",
    "        eval_model = PeftModel.from_pretrained(model, checkpoint)\n",
    "        eval_model = eval_model.to('cuda')\n",
    "\n",
    "        for n, p in eval_model.named_parameters():\n",
    "            if p.device.type == 'cpu':\n",
    "                print(f'{n} is on cpu!')\n",
    "    \n",
    "    else:\n",
    "        print(\"You must set the model_type to base or fine-tuned\")\n",
    "        exit()\n",
    "    \n",
    "    print(f'Proceeding to inference with peft adapters from {checkpoint}')\n",
    "    \n",
    "    eval_model.config.use_cache = True\n",
    "\n",
    "    messages = [\n",
    "        { 'role': 'user', 'content': f\"{user_prompt.strip()}\"},\n",
    "    ]\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    inputs = tokenizer([inputs], return_tensors='pt', add_special_tokens=False).to('cuda')\n",
    "\n",
    "    if \"token_type_ids\" in inputs:\n",
    "        del inputs[\"token_type_ids\"]\n",
    "    \n",
    "    streamer = TextStreamer(tokenizer)\n",
    "    \n",
    "    print(f'eval_model is on: {next(eval_model.parameters()).device}') # Debug line\n",
    "    print(f'input_ids are on: {inputs[\"input_ids\"].device}') # Debug line\n",
    "\n",
    "    # Despite returning the usal output, the streamer will also print the generated \n",
    "    #_ = eval_model.generate(**inputs, streamer=streamer, max_new_tokens=250, do_s)\n",
    "    _ = eval_model.generate(**inputs, streamer=streamer, max_new_tokens=250, do_sample=True)\n",
    "    \n",
    "    # Clear GPU cache and run garbage collection\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "def evaluation(model_type, checkpoint=''):\n",
    "    questions = [\n",
    "        \"What is one plus one?\",\n",
    "        \"Give me some python code to add the first five Fibonacci numbers.\",\n",
    "    ]\n",
    "    \n",
    "    answers = [\n",
    "        \"Two.\",\n",
    "        \"...\",\n",
    "    ]\n",
    "    \n",
    "    for question , answer in zip(questions, answers):\n",
    "        stream(question, model_type, tokenizer, checkpoint)\n",
    "        print('\\n\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "079b652e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"./TinyLlama/TinyLlama_v1.1\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87602f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 2048,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99450377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceeding to inference with peft adapters from LlamaTokenizerFast(name_or_path='./TinyLlama/TinyLlama_v1.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "eval_model is on: cuda:0\n",
      "input_ids are on: cuda:0\n",
      "<s> [INST] What is one plus one? [/INST] IN TH MEMEME ME MEME MEMEME MEME MEMEME ME MEME MEME ME ME MEMEME E ME MEME MEMEME ME ME ME MEMEME MEME MEME MEMEME MEAT ME MEMEME ME DMEME MEMEME MEVER MEME EMEME TEMEME MEMEME MAR MARMAMMAMA MARMAT MARMAMMA MA MR MARMA MR M MAR MR MAR MR M MAR MR M MAR MR MR MR MR MR MR MR MR MR M MR MR MR MR MR MR MR M MR MR MR MR MAR MR MRMR MAR M MARMR MAR MAR MARMA MAR H MAR MARMA MAR Mar MAR MAR MAR MAMA RAT MR M MR MR MR MR MR MAR MAR MAR MA MR MR MR MR MR MRMMMRMMMRMRMMMR MYMA MR MR MR MR MRMRMRMPMRTR MR MRMRMM MR MR MRMA MR MRMR MR MR MRMRMRMRMR MR TUR MR MRMT MRM MRMR M TR MR MR MR MR MR MR MRMRMR MR T MR MR MR MR MR R MR MR MR MR MRMO MR MR MRMR mour MR MR MRMMR MR\n",
      "\n",
      "\n",
      "\n",
      "Proceeding to inference with peft adapters from LlamaTokenizerFast(name_or_path='./TinyLlama/TinyLlama_v1.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "eval_model is on: cuda:0\n",
      "input_ids are on: cuda:0\n",
      "<s> [INST] Give me some python code to add the first five Fibonacci numbers. [/INST]\"]], [\"INFORMATION\"]].\n",
      "[[INFORMATION]]\n",
      "NAME: \"Alexandre\"\n",
      "PIT: -1.0\n",
      "REMARKS: \"French\", \"Dutch\", [\"German\", \"Vietnamese\", \"Scissors, Cut\", \"Teddy bear\", \"French\"]\n",
      "\n",
      "\n",
      "{\n",
      "  \"NAME\": \"Maria\",\n",
      "  \"REMARKS\": \".Japanese\",\n",
      "  \"WITHDRAW\":\n",
      "     {\"MOTHER\": \"\",\n",
      "      \"POTATOES\": \"12\"},\n",
      "  \"CLOSESURE\": {\n",
      "     \"$TYPE\": {\n",
      "       \"MOTHER\": \"'French'\"\n",
      "     },\n",
      "     \"CHILD\": [\n",
      "       {\n",
      "         \"MOTHER\": \"0\",\n",
      "         \"POTATOES\": \"122\"\n",
      "       }]\n",
      "    }\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "</s>\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluation('base', tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e363528",
   "metadata": {},
   "source": [
    "# Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7cbb6ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% if messages[0]['role'] != 'assistant' %}{{ bos_token }}{% endif %}{% for message in messages %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token }}{% endif %}{% endfor %}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0e88003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dcedbe736fc419695aea045621a8331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting comparisons with prompt template:   0%|          | 0/43245 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a300f00903224512b1cedea5948f32b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting comparisons with prompt template:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#¬†Prepared with the help of code from: https://github.com/xfactlab/orpo/blob/main...\n",
    "import json\n",
    "\n",
    "# Load the dataset\n",
    "#dataset_name = 'argilla/dpo-mix-7k' #¬†Ensure this is defined\n",
    "dataset_name = 'mlabonne/orpo-dpo-mix-40k' #¬†Ensure this is defined\n",
    "# dataset_name = 'argilla/OpenHermesPrefenrences' #¬†Ensure this is defined\n",
    "\n",
    "max_num_samples = None #¬†Set to None to use the full dataset\n",
    "# max_num_samples = 1000 #¬†set to None to use the full dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "def build_dataset(tokenizer, data_name, cache_dir=None, max_num_samples=10000, test_split_max=1000):\n",
    "    # Determin the split specification based on max_num samples\n",
    "    split_spec = 'train' if max_num_samples is None else f'train[:{max_num_samples}]'\n",
    "\n",
    "    # Load the dataset\n",
    "    full_data = load_dataset(data_name, split=split_spec, cache_dir=cache_dir)\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    if max_num_samples is not None:\n",
    "        full_data = full_data.shuffle(seed=42)\n",
    "    else:\n",
    "        full_data = full_data\n",
    "\n",
    "    # Determine the number of test samples\n",
    "    num_total_samples = len(full_data)\n",
    "    test_size = min(test_split_max, min(1000, int(0.1 * num_total_samples)))\n",
    "\n",
    "    # Randomly split the data into training and test sets\n",
    "    dataset = full_data.train_test_split(test_size=test_size)\n",
    "\n",
    "    # ds_train = dataset['train']\n",
    "    # ds_test = dataser['test']\n",
    "\n",
    "    column_names = list(dataset['train'].features)\n",
    "\n",
    "    def apply_dpo_template(example):\n",
    "        # function adapted from https://kaitchup.substrack.com/p/fine-tune-a-better-go\n",
    "        if all(k in example.keys() for k in ('chosen', 'rejected')):\n",
    "            # For DPO, the inputs are triples of (prompt, chosen, rejected), where 'chosen'\n",
    "            # We therefore need to extract the N-1 turns to form the prompt\n",
    "            prompt_messages = example['chosen'][:-1]\n",
    "            example['messages'] = example['chosen']\n",
    "\n",
    "            # Now we extract the final turn to define chosen/rejected responses\n",
    "            chosen_messages = example['chosen'][-1:]\n",
    "            rejected_messages = example['rejected'][-1:]\n",
    "            example['text_chosen'] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n",
    "            example['text_rejected'] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n",
    "            example['text_prompt'] = tokenizer.apply_chat_template(prompt_messages, tokenize=False)\n",
    "        return example\n",
    "\n",
    "    dataset = dataset.map(apply_dpo_template, remove_columns=column_names,\n",
    "                desc='Formatting comparisons with prompt template',)\n",
    "\n",
    "    for split in ['train', 'test']:\n",
    "        dataset[split] = dataset[split].rename_columns(\n",
    "            {'text_prompt': 'prompt', 'text_chosen': 'chosen', 'text_rejected': 'rejected', 'messages': 'messages'}\n",
    "        )\n",
    "\n",
    "    return dataset['train'], dataset['test']\n",
    "\n",
    "# Assuming 'tokenizer' and 'dataset_name' are already defined\n",
    "train, test = build_dataset(tokenizer, dataset_name, cache_dir='./dataset', max_num_samples=max_num_samples)\n",
    "\n",
    "# Check the chat template!!! <s> should not be included when tokenizing the respones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5810b26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: <s>[INST] Q: Did Al-Farabi ever meet Mohammed?\n",
      "A: no\n",
      "Explanation: Al-Farabi was born in 872 AD. Mohammed died in 832 AD.\n",
      "\n",
      "Q: Can music be used as a weapon?\n",
      "A: yes\n",
      "Explanation: Music is an art form whose medium is sound. Music can help elevate or subdue emotions. People connect to music through the sound. The military uses loud music to cause psychological disorientation and confusion. The military calls the use of loud disorienting music part of psychological operations.\n",
      "\n",
      "Q: Can you write a whole Haiku in a single tweet?\n",
      "A: [/INST]\n",
      "\n",
      "\n",
      "Chosen: A spring breeze whispers  \n",
      "Through cherry blossoms  \n",
      "Nature's symphony  \n",
      "(5 syllables in the first line, 7 syllables in the second line, 5 syllables in the third line)</s>\n",
      "\n",
      "\n",
      "Rejected: No, 140 characters \n",
      "\n",
      "Unfurl the petal's secret\n",
      "A Haiku, unfolding a rustling dance\n",
      "Here's a delicate answer.</s>\n",
      "\n",
      "\n",
      "Messages (incl. prompt): [{'content': 'Q: Did Al-Farabi ever meet Mohammed?\\nA: no\\nExplanation: Al-Farabi was born in 872 AD. Mohammed died in 832 AD.\\n\\nQ: Can music be used as a weapon?\\nA: yes\\nExplanation: Music is an art form whose medium is sound. Music can help elevate or subdue emotions. People connect to music through the sound. The military uses loud music to cause psychological disorientation and confusion. The military calls the use of loud disorienting music part of psychological operations.\\n\\nQ: Can you write a whole Haiku in a single tweet?\\nA:', 'role': 'user'}, {'content': \"A spring breeze whispers  \\nThrough cherry blossoms  \\nNature's symphony  \\n(5 syllables in the first line, 7 syllables in the second line, 5 syllables in the third line)\", 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "print('Prompt:', train['prompt'][0])\n",
    "print('\\n\\nChosen:', train['chosen'][0])\n",
    "print('\\n\\nRejected:', train['rejected'][0])\n",
    "print('\\n\\nMessages (incl. prompt):', train['messages'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e831b9da",
   "metadata": {},
   "source": [
    "# Train!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77bc43c",
   "metadata": {},
   "source": [
    "## Set up and run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "199f83d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT\n"
     ]
    }
   ],
   "source": [
    "model_name = model_id.split('/')[-1]\n",
    "\n",
    "epochs=1\n",
    "grad_accum=4\n",
    "batch_size=8\n",
    "fine_tune_tag='SFT'\n",
    "save_dir = f'./results/{model_name}_{dataset_name}_{epochs}_epochs_{fine_tune_tag}'\n",
    "print(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dade1990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Custom callback to log metrics\n",
    "class LoggingCallback(transformers.TrainerCallback):\n",
    "    def __init__(self, log_file_path):\n",
    "        self.log_file_path = log_file_path\n",
    "\n",
    "    def on_log(self, args, state, control, model=None, logs=None, **kwargs):\n",
    "        with open(self.log_file_path, 'a') as f:\n",
    "            if 'loss' in logs:\n",
    "                f.write(f'Step: {state.global_step}, Training Loss: {logs[\"loss\"]}\\n')\n",
    "                if 'eval_loss' in logs:\n",
    "                    f.write(f'Step: {state.global_step}, Eval Loss: {logs[\"eval_loss\"]}\\n')\n",
    "                f.flush()  # Force flush the buffered data to file\n",
    "\n",
    "        # Check if the current step is a checkpoint step\n",
    "        if state.global_step % int(args.save_steps) == 0:\n",
    "            # Check if the last checkpoint path exists\n",
    "            if state.best_model_checkpoint:\n",
    "                checkpoint_dir = state.best_model_checkpoint\n",
    "            else:\n",
    "                # If not, construct the checkpoint directory path\n",
    "                checkpoint_dir = os.path.join(args.output_dir, f'checkpoint-{state.global_step}')\n",
    "\n",
    "            # Ensure the checkpoint directory exists\n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "            # Save trainable params in the checkpoint directory\n",
    "            current_trainable_params = {n: p for n, p in model.named_parameters() if p.requires_grad}\n",
    "            current_trainable_params_state_dict = {n: p.data for n, p in current_trainable_params.items()}\n",
    "            file_path = os.path.join(checkpoint_dir, 'trainable_params.pt')\n",
    "            torch.save(current_trainable_params_state_dict, file_path)\n",
    "\n",
    "# Log file path\n",
    "cache_dir = './dataset'  # Assuming cache_dir is defined elsewhere in your code\n",
    "log_file_path = os.path.join(cache_dir, 'training_logs.txt')\n",
    "\n",
    "# Create an instance of the custom callback\n",
    "logging_callback = LoggingCallback(log_file_path)\n",
    "                             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cb687e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a22fc8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 43245\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Remove unnecessary columns\n",
    "train = train.remove_columns(['prompt', 'chosen', 'rejected'])\n",
    "\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "42ec8714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "PyTorch: setting up devices\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ü§ó Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:355: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    # peft_config=peft_config, # commet out if passing a peft model\n",
    "    max_seq_length=1024,\n",
    "    tokenizer=tokenizer, # Trainer uses the chat template passsed by the tokenizer. If data consist only of the column messages, this is fine\n",
    "    model=model,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test,\n",
    "    args=transformers.TrainingArguments(\n",
    "        # max_step=1, # comment this out after the first time you...\n",
    "        save_steps=150, ### MAKE SURE TO CHECK THIS VALUE IS GOOD FOR\n",
    "        logging_steps=1,\n",
    "        num_train_epochs=epochs,\n",
    "        output_dir=save_dir,\n",
    "        eval_strategy='steps',\n",
    "        do_eval=True,\n",
    "        eval_steps=0.2,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        gradient_accumulation_steps=grad_accum,\n",
    "        log_level='debug',\n",
    "        #optim='paged_adamw_8bit',\n",
    "        #fp16=True, # For non-Ampere GPUs\n",
    "        bf16=True, # For Ampere GPUs or later\n",
    "        max_grad_norm=0.3,\n",
    "        lr_scheduler_type='linear',\n",
    "        #hub_private_repo=True,\n",
    "        warmup_ratio=0.03, # optional, may help stability at the (learning rate is ower for the first stepts)\n",
    "        optim='adamw_torch', # comment out for LoRA +\n",
    "        learning_rate=1e-4, # comment out for LoRA +\n",
    "    ),\n",
    "\n",
    "    callbacks=[logging_callback] # Add custom callback here\n",
    "    # neftune_noise_alpha=5 # Aff in noise to embeddings to improve...\n",
    ")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "480a0d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Currently training with a batch size of: 8\n",
      "***** Running training *****\n",
      "  Num examples = 43,245\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 1,351\n",
      "  Number of trainable parameters = 65,628,160\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/_LEARNING/me/FineTuning/wandb/run-20240612_091154-2f6so3xa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/demaz/huggingface/runs/2f6so3xa' target=\"_blank\">./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT</a></strong> to <a href='https://wandb.ai/demaz/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/demaz/huggingface' target=\"_blank\">https://wandb.ai/demaz/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/demaz/huggingface/runs/2f6so3xa' target=\"_blank\">https://wandb.ai/demaz/huggingface/runs/2f6so3xa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1351' max='1351' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1351/1351 1:11:40, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>271</td>\n",
       "      <td>1.450700</td>\n",
       "      <td>1.353275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>542</td>\n",
       "      <td>1.271700</td>\n",
       "      <td>1.335227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>813</td>\n",
       "      <td>1.317600</td>\n",
       "      <td>1.329108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1084</td>\n",
       "      <td>1.406600</td>\n",
       "      <td>1.327019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-150\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "tokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-150/tokenizer_config.json\n",
      "Special tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-150/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-300\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "tokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-300/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-450\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "tokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-450/tokenizer_config.json\n",
      "Special tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-450/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-600\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "tokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-600/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-750\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "tokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-750/tokenizer_config.json\n",
      "Special tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-750/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-900\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "tokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-900/tokenizer_config.json\n",
      "Special tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-900/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1050\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "tokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1050/tokenizer_config.json\n",
      "Special tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1050/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1200\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "tokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1200/tokenizer_config.json\n",
      "Special tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1200/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1350\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "tokenizer config file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1350/tokenizer_config.json\n",
      "Special tokens file saved in ./results/TinyLlama_v1.1_mlabonne/orpo-dpo-mix-40k_1_epochs_SFT/checkpoint-1350/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1351, training_loss=1.3279773617214312, metrics={'train_runtime': 4304.6658, 'train_samples_per_second': 10.046, 'train_steps_per_second': 0.314, 'total_flos': 2.616444791832576e+17, 'train_loss': 1.3279773617214312, 'epoch': 0.9996300406955235})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.use_cache=False # silence the warnings\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5a9d7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./llmat/TinyLlama-1.1B_SFT\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "tokenizer config file saved in ./llmat/TinyLlama-1.1B_SFT/tokenizer_config.json\n",
      "Special tokens file saved in ./llmat/TinyLlama-1.1B_SFT/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d6ee5f",
   "metadata": {},
   "source": [
    "## Plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "949f54c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize lists to hold training and evaluation losses and steps\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "train_steps = []\n",
    "eval_steps = []\n",
    "\n",
    "# Populate the lists from the log history\n",
    "for entry in trainer.state.log_history:\n",
    "    if 'loss' in entry:\n",
    "        train_losses.append(entry['loss'])\n",
    "        train_steps.append(entry['step'])\n",
    "    if 'eval_loss' in entry:\n",
    "        eval_losses.append(entry['eval_loss'])\n",
    "        eval_steps.append(entry['step'])\n",
    "\n",
    "# Plot the losses\n",
    "plt.plot(train_steps, train_losses, label='Train Loss')\n",
    "plt.plot(eval_steps, eval_losses, label='Eval Loss')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566ca8e6",
   "metadata": {},
   "source": [
    "## Save trainable params if training non-LoRA modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a4bc7154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the dictionary to reflect the final state of the model's\n",
    "trainable_params_state_dict = {n: p.data for n, p in model.named_parameters() if p.requires_grad}\n",
    "\n",
    "# Save the final state of the trainable parameters (ONLY RELEVANT FOR NON-LORA ADAPTERS)\n",
    "final_save_path = os.path.join(save_dir, 'trainable_params_final.pt')\n",
    "torch.save(trainable_params_state_dict, final_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2ea5b3",
   "metadata": {},
   "source": [
    "# Evaluate after Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ff5d73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can set to true for faster inference\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "695da28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceeding to inference with peft adapters from LlamaTokenizerFast(name_or_path='./TinyLlama/TinyLlama_v1.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "eval_model is on: cuda:0\n",
      "input_ids are on: cuda:0\n",
      "<s> [INST] What is one plus one? "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[/INST]One plus one is 2. What is 2?</s>\n",
      "\n",
      "\n",
      "\n",
      "Proceeding to inference with peft adapters from LlamaTokenizerFast(name_or_path='./TinyLlama/TinyLlama_v1.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "eval_model is on: cuda:0\n",
      "input_ids are on: cuda:0\n",
      "<s> [INST] Give me some python code to add the first five Fibonacci numbers. [/INST]python code to add the first five Fibonacci numbers:\n",
      "\n",
      "```python\n",
      "fib = 0\n",
      "\n",
      "for i in range(1, 7):\n",
      "   fib += fib\n",
      "\n",
      "print(fib)\n",
      "```\n",
      "\n",
      "This program will print the fibonacci sequence 0, 1, 1, 2, 3, 5, and 8.\n",
      "\n",
      "# Summary\n",
      "\n",
      "In an effort to expand on the basic fibonacci sequence we learn about in 10 of 11, I have written a complete implementation of the fibonacci sequence for Python. \n",
      "\n",
      "While the Python code works exactly the same way as the C code, I wanted to showcase the differences in how the code is written and how functions are created.\n",
      "</s>\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluation('base', tokenizer) # use this if training was done with "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad250fc",
   "metadata": {},
   "source": [
    "# Merge Adapters and Save Model to Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fe313db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU cache cleared\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "del trainer, model\n",
    "gc.collect()\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Clear GPU cache\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU cache cleared\")\n",
    "else:\n",
    "    print(\"CUDA is not available. No GPU cache to clear.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e9557f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./TinyLlama/TinyLlama_v1.1/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"./TinyLlama/TinyLlama_v1.1\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 5632,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 22,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file ./TinyLlama/TinyLlama_v1.1/pytorch_model.bin\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at ./TinyLlama/TinyLlama_v1.1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file ./TinyLlama/TinyLlama_v1.1/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 2048,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reload tokenizer and model\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    #low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Merge adapter with base model\n",
    "model = PeftModel.from_pretrained(model, new_model)\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "af275c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
      "Token has not been saved to git credential helper.\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token $HUGGINGFACE_TOKEN --add-to-git-credential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "40c8210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the save and push paths\n",
    "adapter_model = f\"llmat/{model_name}-{fine_tune_tag}-adapters\"\n",
    "new_model = f\"llmat/{model_name}-{fine_tune_tag}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1a4e1f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2390: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Configuration saved in demaz/TinyLlama_v1.1-SFT-adapters/config.json\n",
      "Configuration saved in demaz/TinyLlama_v1.1-SFT-adapters/generation_config.json\n",
      "Model weights saved in demaz/TinyLlama_v1.1-SFT-adapters/model.safetensors\n",
      "Uploading the following files to llmat/TinyLlama_v1.1-SFT-adapters: config.json,generation_config.json,model.safetensors,README.md\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9beeae148634cfb9e1c273c6a7701e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in demaz/TinyLlama_v1.1-SFT-adapters/tokenizer_config.json\n",
      "Special tokens file saved in demaz/TinyLlama_v1.1-SFT-adapters/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('demaz/TinyLlama_v1.1-SFT-adapters/tokenizer_config.json',\n",
       " 'demaz/TinyLlama_v1.1-SFT-adapters/special_tokens_map.json',\n",
       " 'demaz/TinyLlama_v1.1-SFT-adapters/tokenizer.model',\n",
       " 'demaz/TinyLlama_v1.1-SFT-adapters/added_tokens.json',\n",
       " 'demaz/TinyLlama_v1.1-SFT-adapters/tokenizer.json')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save_pretrained(adapter_model, push_to_hub=True, use_auth_token=True)\n",
    "\n",
    "# Save the tokenizer to make sure the updated config is saved as well\n",
    "tokenizer.save_pretrained(adapter_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "28fdd5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in /tmp/tmpmuhhlq3z/config.json\n",
      "Configuration saved in /tmp/tmpmuhhlq3z/generation_config.json\n",
      "Model weights saved in /tmp/tmpmuhhlq3z/model.safetensors\n",
      "Uploading the following files to llmat/TinyLlama_v1.1-SFT: README.md,generation_config.json,model.safetensors,config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "133b0338515a41c69fe4ccedf2c4ccaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/llmat/TinyLlama_v1.1-SFT/commit/55acc55d8d2ebbb849c7158882497200d05c82aa', commit_message='Upload LlamaForCausalLM', commit_description='', oid='55acc55d8d2ebbb849c7158882497200d05c82aa', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(new_model, use_auth_token=True, max_shard_size='10GB', use_safetensors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7cfb5ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:836: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98fdc202cd5a43eea921c9f133cb2b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in /tmp/tmpnsc3n0cf/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/tmpnsc3n0cf/special_tokens_map.json\n",
      "Uploading the following files to llmat/TinyLlama_v1.1-SFT: tokenizer.json,tokenizer_config.json,README.md,special_tokens_map.json,tokenizer.model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153307bdd71144589af0cf9ab53fc27e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/llmat/TinyLlama_v1.1-SFT/commit/79d8493c8eba87d4355215d4c9e37cdc400210b2', commit_message='Upload tokenizer', commit_description='', oid='79d8493c8eba87d4355215d4c9e37cdc400210b2', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push the tokenizer\n",
    "## OR Reload from scratch if you don't want pad tokens to be in the tokenizer (which you don't if this makes the tokenizer si\n",
    "\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.push_to_hub(new_model, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed80460",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
